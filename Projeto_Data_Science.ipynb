{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "mE4-PIaTAfKX"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ErycaFMS/DS_Projeto/blob/main/Projeto_Data_Science.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1PEQEdZ9zm4"
      },
      "source": [
        "## 1. Definição do Problema"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O dataset utilizado neste projeto é proveniente do sistema SCADA de uma turbina eólica localizada na Turquia. O objetivo é analisar a performance da turbina, monitorando e avaliando a produção de energia e as condições ambientais. Este dataset foi coletado em intervalos de 10 minutos e contém informações cruciais para o entendimento da eficiência da turbina e do impacto das condições do vento sobre a geração de energia. Para mais detalhes sobre este dataset, consulte: https://www.kaggle.com/datasets/berkerisen/wind-turbine-scada-dataset?resource=download&select=T1.csv\n",
        "\n",
        "**Informações sobre os atributos:**\n",
        "\n",
        "1. **Date/Time**: Data e hora em que a medição foi realizada (intervalos de 10 minutos).\n",
        "2. **ActivePower** (kW): Potência ativa gerada pela turbina no momento da medição, em quilowatts (kW).\n",
        "3. **Wind Speed** (m/s): Velocidade do vento na altura do eixo da turbina, medida em metros por segundo (m/s). Esta é a velocidade do vento utilizada para a geração de eletricidade.\n",
        "4. **Theoretical_Power_Curve** (KWh): Valores teóricos de potência que a turbina pode gerar com a velocidade do vento fornecida, conforme especificado pelo fabricante da turbina, medidos em quilowatt-hora (KWh).\n",
        "5. **Wind Direction** (°): Direção do vento na altura do eixo da turbina, medida em graus (°). As turbinas eólicas ajustam-se automaticamente para se alinhar com esta direção."
      ],
      "metadata": {
        "id": "jOuirUlT-fmC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCn8CH4M7wF-"
      },
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import missingno as ms # para tratamento de missings\n",
        "from matplotlib import cm\n",
        "from pandas import set_option\n",
        "from pandas.plotting import scatter_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJpWLh52-aPE"
      },
      "source": [
        "# configuração para não exibir os warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PcB0Efd-MS4"
      },
      "source": [
        "## 2. Carga de Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmHBahfF-cJL"
      },
      "source": [
        "Iremos usar o pacote Pandas (Python Data Analysis Library) para carregar de um arquivo .csv sem cabeçalho disponível online.\n",
        "\n",
        "Com o dataset carregado, iremos explorá-lo um pouco."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega arquivo csv usando Pandas usando uma URL\n",
        "\n",
        "# Informa a URL de importação do dataset\n",
        "url = \"https://raw.githubusercontent.com/ErycaFMS/DS_Projeto/main/T1.csv\"\n",
        "\n",
        "# Informa o cabeçalho das colunas\n",
        "colunas = ['Data/Hora', 'Potência Ativa [kW]', 'Velocidade do Vento [m/s]', 'Potência Teórica [kW]', 'Direção do Vento [°]']\n",
        "\n",
        "# Lê o arquivo utilizando as colunas informadas\n",
        "dataset = pd.read_csv(url, names=colunas, skiprows=1, delimiter=',')\n",
        "\n",
        "# Converte a coluna 'Data/Hora' de string para datetime\n",
        "dataset['Data/Hora'] = pd.to_datetime(dataset['Data/Hora'], format='%d %m %Y %H:%M')"
      ],
      "metadata": {
        "id": "cjIjKDHK8c72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxWtav9TyS1R"
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBSgQt_z_TnV"
      },
      "source": [
        "## 3. Análise de Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqINv-wo_Xfe"
      },
      "source": [
        "### 3.1. Estatísticas Descritivas\n",
        "\n",
        "Vamos iniciar examinando as dimensões do dataset, suas informações e alguns exemplos de linhas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zF3f00Zl_g7j"
      },
      "source": [
        "# Mostra as dimensões do dataset\n",
        "print(dataset.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUjmUTEOwQZt"
      },
      "source": [
        "# Mostra as informações do dataset\n",
        "print(dataset.info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPy9N4FuwX6H"
      },
      "source": [
        "# Mostra as 10 primeiras linhas do dataset\n",
        "dataset.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMyYMlMVwYCS"
      },
      "source": [
        "# Mostra as 10 últimas linhas do dataset\n",
        "dataset.tail(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY7gaBfqwdy5"
      },
      "source": [
        "É sempre importante verificar o tipo do atributos do dataset, pois pode ser necessário realizar conversões. Já fizemos anteriormente com o comando info, mas vamos ver uma outra forma de verificar a natureza de cada atributo e então exibir um resumo estatístico do dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dRVheWE_mJQ"
      },
      "source": [
        "# Verifica o tipo de dataset de cada atributo\n",
        "dataset.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NKdYewownzG"
      },
      "source": [
        "# Faz um resumo estatístico do dataset (média, desvio padrão, mínimo, máximo e os quartis)\n",
        "dataset.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notamos que o valor mínimo de Potência Ativa é negativo, evidenciando futura necessidade de tratamento de dados."
      ],
      "metadata": {
        "id": "sXAhNZXHrLkB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_5Ntt3f_tTv"
      },
      "source": [
        "### 3.2. Visualizações Unimodais\n",
        "\n",
        "Vamos criar agora um histograma para cada atributo do dataset. Veremos que o atributo VelVento segue uma distribuição aproximadamente normal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dK3mK65T_tYx"
      },
      "source": [
        "# Histograma\n",
        "dataset.hist(figsize = (15,10))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kObO1uuU_teC"
      },
      "source": [
        "O Gráfico de Densidade, ou Density Plot, é bem parecido com o histograma, mas com uma visualização um pouco diferente. Com ele, pode ser mais fácil identificar a distribuição do atributos do dataset. Assim como fizemos com o histograma, vamos criar um density plot para cada atributo do dataset.\n",
        "\n",
        "Veremos que muitos dos atributos têm uma distribuição distorcida. Uma transformação como a Box-Cox, que pode aproximar a distribuição de uma Normal, pode ser útil neste caso."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHUaJJNK_tji"
      },
      "source": [
        "# Remove a coluna 'Data/Hora' para o plot de densidade\n",
        "dataset_numerico = dataset.drop(columns=['Data/Hora'])\n",
        "\n",
        "# Density Plot\n",
        "dataset_numerico.plot(kind = 'density', subplots = True, layout = (3,3), sharex = False, figsize = (15,10))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08IaXUcCxuj6"
      },
      "source": [
        "Vamos agora trabalhar com boxplots. No **boxblot**, a linha no centro (vermelha) representa o valor da mediana (segundo quartil ou p50). A linha abaixo é o 1o quartil (p25) e a linha acima o terceiro quartil (p75). O boxplot ajuda a ter uma ideia da dispersão dos dataset e os possíveis outliers.\n",
        "\n",
        "*OBS: Se um ponto do dataset é muito distante da média (acima de 3 desvios padrão da média), pode ser considerado outlier.*\n",
        "\n",
        "Nos gráficos bloxplot, veremos que a dispersão dos atributos do dataset é bem diferente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ctc5ftKgxurF"
      },
      "source": [
        "# Boxplot\n",
        "dataset.plot(kind = 'box', subplots = True, layout = (3,3), sharex = False, sharey = False, figsize = (15,10))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMMGbjYbAe-3"
      },
      "source": [
        "### 3.3. Visualizações Multimodais\n",
        "\n",
        "Ao visualizar as correlações entre os atributos através da matriz de correlação, perceberemos que parece haver alguma estrutura na ordem dos atributos. O azul ao redor da diagonal sugere que os atributos que estão próximos um do outro são geralmente mais correlacionados entre si. Os vermelhos também sugerem alguma correlação negativa moderada, a medida que os atributos\n",
        "\n",
        "Vamos agora verificar a covariância entre as variáveis numéricas do dataset. A **covariância** representa como duas variáveis numéricas estão relacionadas. Existem várias formas de calcular a correlação entre duas variáveis, como por exemplo, o coeficiente de correlação de Pearson, que pode ser:\n",
        "* Próximo de -1 : há uma correlação negativa entre as variáveis,\n",
        "* Próximo de +1: há uma correlação positiva entre as variáveis.\n",
        "* 0: não há correlação entre as variáveis.\n",
        "\n",
        "<i>OBS: Esta informação é relevante porque alguns algoritmos como regressão linear e regressão logística podem apresentar problemas de performance se houver atributos altamente correlacionados. Vale a pena consultar a documentação do algoritmo para verificar se algum tipo de tratamento de dataset é necessário.</i>\n",
        "\n",
        "Falamos anteriormente da importância da correlação entre os atributos, e agora iremos visualizar esta informação em formato gráfico. A **matriz de correlação** exibe graficamente a correlação entre os atributos numéricos do dataset.estão mais distantes um do outro na ordenação.\n",
        "\n",
        "O código a seguir exibe a matriz de correlação."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xB612g0aAfE6"
      },
      "source": [
        "# Matriz de Correlação com Matplotlib Seaborn\n",
        "sns.heatmap(dataset.corr(), annot=True, cmap='RdBu');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXCUyGj3y-1F"
      },
      "source": [
        "Por sua vez, o gráfico de dispersão (**scatter plot**) mostra o relacionamento entre duas variáveis. Vamos exibir um para cada par de atributos dos dataset, usando o Seaborn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhHGrt91y-7T"
      },
      "source": [
        "# Scatter Plot com Seaborn - Variação 1\n",
        "\n",
        "sns.pairplot(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mE4-PIaTAfKX"
      },
      "source": [
        "## 4. Preparação dos dados\n",
        "\n",
        "Nesta etapa, poderíamos realizar diversas operações de preparação de dados, como por exemplo, tratamento de valores missings (faltantes), limpeza de dados, transformações como one-hot-encoding, seleção de características (feature selection), entre outras não mostradas neste notebook. Lembre-se de não criar uma versão padronizada/normalizada dos dados neste momento (apesar de serem operações de pré-processamento) para evitar o Data Leakage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REo1UJQZLuFV"
      },
      "source": [
        "### 4.1. Tratamento de Missings e Limpeza"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhAz17RD6YP4"
      },
      "source": [
        "Devemos realizar uma tratamento de dados para os valores de Potência que aparecem negativos, além dos missings. Vamos então fazer este tratamento e criar uma nova visão do nosso dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Utzu_kmLwWi"
      },
      "source": [
        "# verificando nulls no dataset\n",
        "dataset.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Criando cópia do Dataset\n",
        "DatasetAntes = dataset.copy()"
      ],
      "metadata": {
        "id": "CbJtTD-iQ2zW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUJ5YgRWLzjf"
      },
      "source": [
        "# salvando um NOVO dataset para tratamento de missings (para não sobrescrever o dataset original foi criado DatasetAntes)\n",
        "\n",
        "# recuperando os nomes das colunas\n",
        "col = list(DatasetAntes.columns)\n",
        "\n",
        "# substituindo os zeros por NaN\n",
        "DatasetAntes.replace(0, np.nan, inplace=True)\n",
        "\n",
        "# exibindo visualização matricial da nulidade do dataset\n",
        "ms.matrix(DatasetAntes)\n",
        "\n",
        "print(DatasetAntes.info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JdS5PS3MFNo"
      },
      "source": [
        "# removendo a coluna Data/Hora\n",
        "DatasetAntes.drop(['Data/Hora'], axis=1, inplace= True)\n",
        "\n",
        "# exibindo visualização matricial da nulidade do dataset\n",
        "ms.matrix(DatasetAntes)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gerando o gráfico de Potência Ativa e Velocidade do vento, para fins comparativos."
      ],
      "metadata": {
        "id": "Z_GnJLk5L2v3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotando o gráfico potência ativa x velocidade do vento\n",
        "plt.figure(figsize=(15, 10))  # Define o tamanho da figura\n",
        "sns.pairplot(DatasetAntes, x_vars='Velocidade do Vento [m/s]', y_vars='Potência Ativa [kW]', height=6)  # Ajusta o tamanho dos gráficos\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TjjDOZh8L2Oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando Pipeline de Limpeza de dados:\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Função para substituir valores negativos por zero\n",
        "def correct_negative_values(X):\n",
        "    X['Potência Ativa [kW]'] = X['Potência Ativa [kW]'].apply(lambda x: max(x, 0))\n",
        "    return X\n"
      ],
      "metadata": {
        "id": "VMkucMSwR0kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para interpolar valores faltantes e suavizar outliers\n",
        "def interpolate_and_smooth(X):\n",
        "    X['Potência Ativa [kW]'] = X['Potência Ativa [kW]'].interpolate(method='polynomial', order=2)\n",
        "    X['Velocidade do Vento [m/s]'] = X['Velocidade do Vento [m/s]'].interpolate(method='polynomial', order=2)\n",
        "    return X\n",
        "\n",
        "    # Definindo as colunas numéricas e categóricas\n",
        "numeric_features = ['Potência Ativa [kW]', 'Velocidade do Vento [m/s]', 'Potência Teórica [kW]']\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),  # Imputar valores faltantes com a média\n",
        "    ('scaler', StandardScaler())  # Escalar os dados numéricos\n",
        "])\n",
        "\n",
        "categorical_features = ['Direção do Vento [°]']\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Imputar valores categóricos com o mais frequente\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # Codificar as categorias\n",
        "])\n",
        "\n",
        "# Combinando as transformações com as funções personalizadas\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Definindo a pipeline completa\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('negative_correction', FunctionTransformer(correct_negative_values)),\n",
        "    ('interpolation', FunctionTransformer(interpolate_and_smooth)),\n",
        "    ('preprocessor', preprocessor)\n",
        "])\n",
        "\n",
        "# Aplicação da pipeline no DataFrame\n",
        "DatasetAntes = pd.DataFrame(DatasetAntes)\n",
        "pipeline.fit_transform(dataset)"
      ],
      "metadata": {
        "id": "YCUuQrs3TQdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verificando o gráfico antes x depois"
      ],
      "metadata": {
        "id": "-l8BL8HOFc1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicar a pipeline ao dataset\n",
        "dataset_limpo = pipeline.fit_transform(DatasetAntes)\n",
        "\n",
        "# Como o resultado da pipeline é um array numpy, precisamos recriar um DataFrame\n",
        "# Para isso, mantemos as mesmas colunas que foram transformadas\n",
        "dataset_limpo = pd.DataFrame(dataset_limpo, columns=numeric_features + list(pipeline.named_steps['preprocessor'].transformers_[1][1].named_steps['onehot'].get_feature_names_out(categorical_features)))\n",
        "\n",
        "# Plotando o gráfico potência ativa x velocidade do vento após limpeza\n",
        "plt.figure(figsize=(15, 10))  # Define o tamanho da figura\n",
        "sns.pairplot(dataset_limpo, x_vars='Velocidade do Vento [m/s]', y_vars='Potência Ativa [kW]', height=6)  # Ajusta o tamanho dos gráficos\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8FUbL-MRFhGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoLQBjKl9JVD"
      },
      "source": [
        "### 4.2. Separação em conjunto de treino e conjunto de teste\n",
        "\n",
        "É uma boa prática usar um conjunto de teste (na literatura também chamado de conjunto de validação), uma amostra dos dados que não será usada para a construção do modelo, mas somente no fim do projeto para confirmar a precisão do modelo final. É um teste que podemos usar para verificar o quão boa foi a construção do modelo, e para nos dar uma ideia de como o modelo irá performar nas estimativas em dados não vistos. Usaremos 80% do conjunto de dados para modelagem e guardaremos 20% para teste, usando a estratégia train-test-split, já explicada anteriormente. Primeiramente, iremos sinalizar quais são as colunas de atributos (X - 0 a 7) e qual é a coluna das classes (Y - 8). Em seguida, especificaremos o tamanho do conjunto de teste desejado e uma semente (para garantir a reprodutibilidade dos resultados). Finalmente, faremos a separação dos conjuntos de treino e teste através do comando train_test_split, que retornará 4 estruturas de dados: os atributos e classes para o conjunto de teste e os atributos e classes para o conjunto de treino.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEiAm3LEAfPt"
      },
      "source": [
        "# Selecionando as variáveis\n",
        "X = dataset[['Velocidade do Vento [m/s]', 'Potência Teórica [kW]', 'Direção do Vento [°]']]\n",
        "y = dataset['Potência Ativa [kW]']\n",
        "\n",
        "# Separar os dados em treino e teste (80% para treino, 20% para teste)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3. Definindo os Modelos e Parâmetros"
      ],
      "metadata": {
        "id": "yblUxDvrMeQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definindo os modelos e seus hiperparâmetros\n",
        "modelos_params = {\n",
        "    'Regressão Linear': {\n",
        "        'modelo': LinearRegression(),\n",
        "        'params': {}  # Regressão Linear não tem hiperparâmetros para ajuste básico\n",
        "    },\n",
        "    'Árvore de Regressão': {\n",
        "        'modelo': DecisionTreeRegressor(),\n",
        "        'params': {\n",
        "            'classifier__max_depth': [None, 5, 10],\n",
        "            'classifier__min_samples_split': [2, 5, 10]\n",
        "        }\n",
        "    },\n",
        "    'SVR': {\n",
        "        'modelo': SVR(),\n",
        "        'params': {\n",
        "            'classifier__C': [0.1, 1, 10],\n",
        "            'classifier__kernel': ['linear', 'rbf'],\n",
        "            'classifier__epsilon': [0.1, 0.2, 0.5]\n",
        "        }\n",
        "    },\n",
        "    'KNN Regressor': {\n",
        "        'modelo': KNeighborsRegressor(),\n",
        "        'params': {\n",
        "            'classifier__n_neighbors': [3, 5, 7],\n",
        "            'classifier__weights': ['uniform', 'distance']\n",
        "        }\n",
        "    },\n",
        "    'Gradient Boosting Regressor': {\n",
        "        'modelo': GradientBoostingRegressor(),\n",
        "        'params': {\n",
        "            'classifier__n_estimators': [50, 100, 200],\n",
        "            'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
        "            'classifier__max_depth': [3, 5, 7]\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "rSw6CMTJDwld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4. Executando o Grid Search com Validação Cruzada"
      ],
      "metadata": {
        "id": "l_5h2d2AMvl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir o pré-processador (padronização)\n",
        "preprocessor = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Lista para armazenar resultados\n",
        "resultados = []\n",
        "\n",
        "# Executando o Grid Search com validação cruzada para cada modelo\n",
        "for nome, mp in modelos_params.items():\n",
        "    modelo = mp['modelo']\n",
        "    params = mp['params']\n",
        "\n",
        "    # Criando o pipeline com o modelo atual\n",
        "    pipeline = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', modelo)\n",
        "    ])\n",
        "\n",
        "    # Grid Search com validação cruzada\n",
        "    grid_search = GridSearchCV(\n",
        "        estimator=pipeline,\n",
        "        param_grid=params,\n",
        "        cv=5,\n",
        "        scoring='neg_mean_squared_error',\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # Treinando o modelo\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Melhor modelo encontrado\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "    # Fazendo previsões no conjunto de teste\n",
        "    y_pred = best_model.predict(X_test)\n",
        "\n",
        "    # Calculando o erro quadrático médio (MSE)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "    # Armazenando os resultados\n",
        "    resultados.append({\n",
        "        'Modelo': nome,\n",
        "        'Melhores Hiperparâmetros': grid_search.best_params_,\n",
        "        'Erro Quadrático Médio (MSE)': mse,\n",
        "        'Pipeline': best_model  # Armazenando o melhor modelo\n",
        "    })\n",
        "\n",
        "# Exibindo os resultados\n",
        "for resultado in resultados:\n",
        "    print(f\"Modelo: {resultado['Modelo']}\")\n",
        "    print(f\"Melhores Hiperparâmetros: {resultado['Melhores Hiperparâmetros']}\")\n",
        "    print(f\"Erro Quadrático Médio (MSE): {resultado['Erro Quadrático Médio (MSE)']}\\n\")"
      ],
      "metadata": {
        "id": "SAu27bCQM6I8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.5. Comparando os Modelos Otimizados"
      ],
      "metadata": {
        "id": "jF-XpYObPw_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparando os modelos otimizados\n",
        "df_resultados = pd.DataFrame(resultados)\n",
        "\n",
        "# Ordenando os resultados pelo menor Erro Quadrático Médio (MSE)\n",
        "df_resultados = df_resultados.sort_values(by='Erro Quadrático Médio (MSE)', ascending=True)\n",
        "\n",
        "# Resetando o índice\n",
        "df_resultados.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Exibindo as colunas desejadas\n",
        "df_resultados[['Modelo', 'Erro Quadrático Médio (MSE)', 'Melhores Hiperparâmetros']]"
      ],
      "metadata": {
        "id": "YUaIYDYnQA91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.6. Selecionando o melhor modelo"
      ],
      "metadata": {
        "id": "1yEu8mJrQ0ap"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b4b8wu49Q5a4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.7. Salvando o modelo"
      ],
      "metadata": {
        "id": "pVcJ0CtjQ6MR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SiOYVF9eQ_2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Carregando o modelo e fazendo previsões"
      ],
      "metadata": {
        "id": "zFEIlkUNRARb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KlnGo-lwRIZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cBC6_dDRRJXv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusão\n",
        "\n",
        "Escreva aqui os seu principais achados, pontos de atenção e conclusões desse projeto."
      ],
      "metadata": {
        "id": "tWaOST1a-R_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Referências\n",
        "\n",
        "- [Documentação do Scikit-Learn sobre Pipelines](https://scikit-learn.org/stable/modules/compose.html#pipeline)\n",
        "- [Documentação do GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
        "- [Salvando Modelos com Joblib](https://scikit-learn.org/stable/modules/model_persistence.html)\n",
        "- [Dataset Titanic no Kaggle](https://www.kaggle.com/c/titanic/data)\n",
        "- [Validação Cruzada no Scikit-Learn](https://scikit-learn.org/stable/modules/cross_validation.html)\n",
        "- [Métricas de Avaliação de Classificação](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics)"
      ],
      "metadata": {
        "id": "OS4IPSAmRan_"
      }
    }
  ]
}